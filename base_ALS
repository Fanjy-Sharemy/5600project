from pyspark.sql import SparkSession
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.recommendation import ALS
from pyspark.sql import Row
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator

spark = SparkSession.builder.appName('myproj').getOrCreate() #project name

#read ratings
ratings = spark.read.csv('ml-25m/ratings.csv', inferSchema=True, header=True)
ratings.createOrReplaceTempView("ratings")
# result = spark.sql("select * from ratings")
# result.show()

#read movies
movies = spark.read.csv('ml-25m/movies.csv', inferSchema=True, header=True)
movies.createOrReplaceTempView("movies")
moives_df = movies.toPandas()
ratings.groupby('movieid').count().show()

ratings = ratings.select(ratings.userId,
                         ratings.movieId,
                         ratings.rating.cast("double"))

# Count the total number of ratings in the dataset
numerator = ratings.select("rating").count()

# Count the number of distinct Id's
num_users = ratings.select("userId").distinct().count()
num_items = ratings.select("movieId").distinct().count()

# Set the denominator equal to the number of users multiplied by the number of items
denominator = num_users * num_items

# Divide the numerator by the denominator
sparsity = (1.0 - (numerator * 1.0)/ denominator) * 100
print("The ratings dataframe is ", "%.2f" % sparsity + "% empty.")

#als part
(training, test) = ratings.randomSplit([0.8, 0.2])
als = ALS( userCol="userId", itemCol="movieId", ratingCol="rating",
          coldStartStrategy="drop", nonnegative = True, implicitPrefs = False)
param_grid = ParamGridBuilder() \
            .addGrid(als.rank, [10, 50, 75, 100]) \
            .addGrid(als.maxIter, [5, 50, 75, 100]) \
            .addGrid(als.regParam, [.01, .05, .1, .15]) \
            .build()

# Define evaluator as RMSE
evaluator = RegressionEvaluator(metricName = "rmse",
                                labelCol = "rating",
                                predictionCol = "prediction")
# Print length of evaluator
print ("Num models to be tested using param_grid: ", len(param_grid))

# Build cross validation using CrossValidator
cv = CrossValidator(estimator = als,
                    estimatorParamMaps = param_grid,
                    evaluator = evaluator,
                    numFolds = 5)

#fit the model
model = als.fit(training)
predictions = model.transform(test)
predictions.createOrReplaceTempView("predictions")
# result = spark.sql("select * from predictions")
# result.show()

rmse = evaluator.evaluate(predictions)
print("Root-mean-square error = " + str(rmse))

# Generate n recommendations for all users
ALS_recommendations = model.recommendForAllUsers(numItems = 10) # n - 10
ALS_recommendations.show()

# result = spark.sql("select title from movies where movieid = '69670'")
# result.show()

ALS_recommendations.registerTempTable("ALS_recs_temp")
clean_recs = spark.sql("""SELECT userId,
                            movieIds_and_ratings.movieId AS movieId,
                            movieIds_and_ratings.rating AS prediction
                        FROM ALS_recs_temp
                        LATERAL VIEW explode(recommendations) exploded_table
                            AS movieIds_and_ratings""")
clean_recs.show()

# result = spark.sql("select * from predictions order by prediction desc")
# result.show()

new_movies = (clean_recs.join(ratings, ["userId", "movieId"], "left")
    .filter(ratings.rating.isNull()))
# new_movies.show()
new_movies_df = new_movies.toPandas()
new_movies_df.loc[new_movies_df['prediction'] >= 5] = 5
